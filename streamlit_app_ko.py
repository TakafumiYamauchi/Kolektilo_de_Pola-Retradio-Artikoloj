#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlit ì•±(KR): Pola Retradio ê¸°ì‚¬ ìˆ˜ì§‘Â·í†µí•©
ì‹¤í–‰:
    streamlit run GPT5pro/streamlit_app_ko.py
"""
import time
import pandas as pd
import streamlit as st
from datetime import date, timedelta, datetime

from retradio_lib import (
    ScrapeConfig,
    collect_urls,
    fetch_article,
    _session,
    to_markdown,
    to_text,
    to_csv,
    to_jsonl,
)

st.set_page_config(page_title="Pola Retradio ê¸°ì‚¬ ìˆ˜ì§‘Â·í†µí•©", layout="wide")

st.title("ğŸ“» Pola Retradio ê¸°ì‚¬ ìˆ˜ì§‘Â·í†µí•© ë„êµ¬")
st.markdown(
    """
ì§€ì •í•œ ê¸°ê°„ ë™ì•ˆ https://pola-retradio.org/ ì— ê³µê°œëœ ê¸€ì˜ URLê³¼ ë³¸ë¬¸(ì—ìŠ¤í˜ë€í† )ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ í†µí•©í•©ë‹ˆë‹¤.
- ë°©ë²•: RSS/í”¼ë“œì™€ ì›”ë³„ ì•„ì¹´ì´ë¸Œ HTMLì„ ë³‘í–‰ ì‚¬ìš©(í•„ìš” ì‹œ ì „í™˜)
- ì¶œë ¥: Markdown / TXT / CSV / JSONL (ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
- ë°°ë ¤: robots.txt ì¤€ìˆ˜, ì„œë²„ ë¶€í•˜ ìµœì†Œí™”, ì‚¬ëŒìš© UA, ìš”ì²­ ê°„ ì§€ì—°(throttle)
"""
)

min_supported = date(2011, 1, 1)
today = date.today()
default_start = max(min_supported, today - timedelta(days=14))

col1, col2, col3 = st.columns(3)
with col1:
    start = st.date_input(
        "ì‹œì‘ì¼",
        value=default_start,
        min_value=min_supported,
        max_value=today,
    )
with col2:
    end = st.date_input(
        "ì¢…ë£Œì¼",
        value=today,
        min_value=min_supported,
        max_value=today,
    )
with col3:
    method = st.selectbox("ìˆ˜ì§‘ ë°©ë²•", options=["both", "feed", "archive"], index=0)

throttle = st.slider("ìš”ì²­ ê°„ ê°„ê²©(ì´ˆ)", min_value=0.0, max_value=5.0, value=1.0, step=0.1)
max_pages = st.number_input("í˜ì´ì§€ ë„˜ê¹€ ìƒí•œ(None=ì œí•œ ì—†ìŒ)", min_value=0, value=0, step=1)
include_audio = st.checkbox("ì˜¤ë””ì˜¤ ë§í¬ë„ í•¨ê»˜ í‘œê¸°(ì„ íƒ)", value=False)

if st.button("ìˆ˜ì§‘ ì‹¤í–‰", type="primary"):
    cfg = ScrapeConfig(
        start_date=start,
        end_date=end,
        method=method,
        throttle_sec=throttle,
        max_pages=(None if max_pages == 0 else int(max_pages)),
        include_audio_links=include_audio,
        use_cache=True,
    )
    with st.spinner("URL ìˆ˜ì§‘ ì¤‘..."):
        result = collect_urls(cfg)
    urls = result.urls
    st.success(f"í›„ë³´ URL: {result.total}ê±´")
    st.caption(
        f"feed {result.feed_used}/{result.feed_initial}, archive {result.archive_used}/{result.archive_initial}, "
        f"ì¤‘ë³µ ì œê±° {result.duplicates_removed}, ê¸°ê°„ ì™¸ ì œì™¸ {result.out_of_range_skipped}"
    )
    if result.earliest_date and result.latest_date:
        st.caption(f"ì¶”ì • ê³µê°œì¼ ë²”ìœ„: {result.earliest_date} ~ {result.latest_date}")
    st.write("ë³¸ë¬¸ ìˆ˜ì§‘ì—ëŠ” ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤. ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì§€ì—°ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.")

    arts = []
    s = _session(cfg)
    progress = st.progress(0.0, "ë³¸ë¬¸ì„ ìˆ˜ì§‘ ì¤‘...")
    failures = []
    for i, u in enumerate(urls, 1):
        try:
            a = fetch_article(u, cfg, s)
            if a.published and not (cfg.start_date <= a.published.date() <= cfg.end_date):
                pass
            else:
                arts.append(a)
        except Exception as e:
            st.warning(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {u} ({e})")
            failures.append(f"{u} ({e})")
        finally:
            time.sleep(cfg.throttle_sec)
            progress.progress(i / len(urls), f"ë³¸ë¬¸ì„ ìˆ˜ì§‘ ì¤‘... {i}/{len(urls)}")

    def sort_key(a):
        if a.published:
            pub_naive = a.published.replace(tzinfo=None) if a.published.tzinfo else a.published
            return (pub_naive, a.url)
        return (datetime.max, a.url)

    arts.sort(key=sort_key)
    st.success(f"ì¶”ì¶œ ì™„ë£Œ: {len(arts)}ê±´")
    if not arts:
        st.stop()
    if failures:
        with st.expander("ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê¸€(ì¬ì‹œë„ í•„ìš”)"):
            for failure in failures:
                st.write(failure)

    df = pd.DataFrame(
        [
            {
                "ê³µê°œì¼": (a.published.strftime("%Y-%m-%d") if a.published else ""),
                "ì œëª©": a.title,
                "URL": a.url,
                "ì‘ì„±ì": a.author or "",
                "ì¹´í…Œê³ ë¦¬": ", ".join(a.categories or []),
            }
            for a in arts
        ]
    )
    st.dataframe(df, use_container_width=True, hide_index=True)

    md = to_markdown(arts, cfg)
    txt = to_text(arts)
    csv_str = to_csv(arts)
    jsonl = to_jsonl(arts)

    st.download_button("ğŸ“„ Markdown ë‹¤ìš´ë¡œë“œ", md, file_name=f"pola_retradio_{start}_{end}.md", mime="text/markdown")
    st.download_button("ğŸ—’ï¸ TXT ë‹¤ìš´ë¡œë“œ", txt, file_name=f"pola_retradio_{start}_{end}.txt", mime="text/plain")
    st.download_button("ğŸ§¾ CSV ë‹¤ìš´ë¡œë“œ", csv_str, file_name=f"pola_retradio_{start}_{end}.csv", mime="text/csv")
    st.download_button("ğŸ§° JSONL ë‹¤ìš´ë¡œë“œ", jsonl, file_name=f"pola_retradio_{start}_{end}.jsonl", mime="application/json")
else:
    st.info("ì‹œì‘ì¼Â·ì¢…ë£Œì¼ì„ ì„ íƒí•œ ë’¤ â€˜ìˆ˜ì§‘ ì‹¤í–‰â€™ì„ í´ë¦­í•˜ì„¸ìš”.")
