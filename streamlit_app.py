# -*- coding: utf-8 -*-
"""
Streamlit ã‚¢ãƒ—ãƒªï¼šPola Retradio ã®è¨˜äº‹ã‚’æœŸé–“æŒ‡å®šã§åé›†ãƒ»çµ±åˆã—ã€ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
èµ·å‹•:
    streamlit run streamlit_app.py
"""
import time
import pandas as pd
import streamlit as st
from datetime import date, timedelta, datetime

from retradio_lib import (
    ScrapeConfig,
    collect_urls,
    fetch_article,
    _session,
    to_markdown,
    to_text,
    to_csv,
    to_jsonl,
)

st.set_page_config(page_title="Pola Retradio æœŸé–“åé›†ãƒ„ãƒ¼ãƒ«", layout="wide")

st.title("ğŸ“» Pola Retradio è¨˜äº‹åé›†ãƒ»çµ±åˆãƒ„ãƒ¼ãƒ«")
st.markdown("""
æŒ‡å®šã—ãŸæœŸé–“å†…ã« https://pola-retradio.org/ ã§å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã® URL ã¨æœ¬æ–‡ï¼ˆã‚¨ã‚¹ãƒšãƒ©ãƒ³ãƒˆï¼‰ã‚’è‡ªå‹•åé›†ã—ã€1ã¤ã®æ–‡æ›¸ã¸çµ±åˆã—ã¾ã™ã€‚
- **æ–¹æ³•**: RSS/Feed ã¨ æœˆåˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã® HTML ã‚’ä½µç”¨ï¼ˆå¿…è¦ã«å¿œã˜ã¦åˆ‡æ›¿ï¼‰
- **å‡ºåŠ›**: Markdown / TXT / CSV / JSONLï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ï¼‰
- **é…æ…®**: robots.txtãƒ»ä½è² è·ãƒ»äººé–“ UAã€é…å»¶ï¼ˆthrottleï¼‰
""")

col1, col2, col3 = st.columns(3)
with col1:
    start = st.date_input("é–‹å§‹æ—¥", value=(date.today() - timedelta(days=14)))
with col2:
    end = st.date_input("çµ‚äº†æ—¥", value=date.today())
with col3:
    method = st.selectbox("åé›†æ–¹æ³•", options=["both","feed","archive"], index=0)

throttle = st.slider("ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰", min_value=0.0, max_value=5.0, value=1.0, step=0.1)
max_pages = st.number_input("ãƒšãƒ¼ã‚¸é€ã‚Šã®ä¸Šé™ï¼ˆNone=åˆ¶é™ãªã—ï¼‰", min_value=0, value=0, step=1)
include_audio = st.checkbox("éŸ³å£°ãƒªãƒ³ã‚¯ã‚‚ä½µè¨˜ã™ã‚‹ï¼ˆä»»æ„ï¼‰", value=False)

if st.button("åé›†ã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
    cfg = ScrapeConfig(
        start_date=start, end_date=end, method=method,
        throttle_sec=throttle, max_pages=(None if max_pages==0 else int(max_pages)),
        include_audio_links=include_audio, use_cache=True
    )
    with st.spinner("URL ã‚’åé›†ä¸­..."):
        result = collect_urls(cfg)
    urls = result.urls
    st.success(f"å€™è£œ URL: {result.total} ä»¶")
    st.caption(
        f"feed {result.feed_used}/{result.feed_initial}, archive {result.archive_used}/{result.archive_initial}, "
        f"duplicates removed {result.duplicates_removed}, out-of-range skipped {result.out_of_range_skipped}"
    )
    if result.earliest_date and result.latest_date:
        st.caption(f"æ¨å®šå…¬é–‹æ—¥ç¯„å›²: {result.earliest_date} ï½ {result.latest_date}")
    st.write("ï¼ˆæœ¬æ–‡å–å¾—ã«ã¯å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ä½è² è·ã®ãŸã‚é…å»¶ã‚’å…¥ã‚Œã¦ã„ã¾ã™ã€‚ï¼‰")

    arts = []
    s = _session(cfg)
    progress = st.progress(0.0, "æœ¬æ–‡ã‚’å–å¾—ä¸­...")
    failures = []
    for i, u in enumerate(urls, 1):
        try:
            a = fetch_article(u, cfg, s)
            if a.published and not (cfg.start_date <= a.published.date() <= cfg.end_date):
                # ç¯„å›²å¤–ã¯é™¤å¤–
                pass
            else:
                arts.append(a)
        except Exception as e:
            st.warning(f"å–å¾—å¤±æ•—: {u} ({e})")
            failures.append(f"{u} ({e})")
        finally:
            time.sleep(cfg.throttle_sec)
            progress.progress(i/len(urls), f"æœ¬æ–‡ã‚’å–å¾—ä¸­... {i}/{len(urls)}")

    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’å‰Šé™¤ã—ã¦æ¯”è¼ƒ
    def sort_key(a):
        if a.published:
            pub_naive = a.published.replace(tzinfo=None) if a.published.tzinfo else a.published
            return (pub_naive, a.url)
        return (datetime.max, a.url)
    arts.sort(key=sort_key)
    st.success(f"æŠ½å‡ºå®Œäº†: {len(arts)} æœ¬")
    if not arts:
        st.stop()
    if failures:
        with st.expander("å–å¾—ã§ããªã‹ã£ãŸè¨˜äº‹ï¼ˆè¦å†è©¦è¡Œï¼‰"):
            for failure in failures:
                st.write(failure)

    # è¡¨ç¤º
    df = pd.DataFrame([{
        "published": (a.published.strftime("%Y-%m-%d") if a.published else ""),
        "title": a.title, "url": a.url,
        "author": a.author or "", "categories": ", ".join(a.categories or [])
    } for a in arts])
    st.dataframe(df, use_container_width=True, hide_index=True)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    md = to_markdown(arts, cfg)
    txt = to_text(arts)
    csv_str = to_csv(arts)
    jsonl = to_jsonl(arts)

    st.download_button("ğŸ“„ Markdown ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", md, file_name=f"pola_retradio_{start}_{end}.md", mime="text/markdown")
    st.download_button("ğŸ—’ï¸ TXT ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", txt, file_name=f"pola_retradio_{start}_{end}.txt", mime="text/plain")
    st.download_button("ğŸ§¾ CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_str, file_name=f"pola_retradio_{start}_{end}.csv", mime="text/csv")
    st.download_button("ğŸ§° JSONL ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", jsonl, file_name=f"pola_retradio_{start}_{end}.jsonl", mime="application/json")
else:
    st.info("é–‹å§‹æ—¥ãƒ»çµ‚äº†æ—¥ã‚’é¸ã‚“ã§ã€Œåé›†ã‚’å®Ÿè¡Œã™ã‚‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
