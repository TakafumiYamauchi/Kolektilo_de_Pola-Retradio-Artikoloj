#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlit apo (EO): Ilo por kolekti kaj kunigi artikolojn de Pola Retradio
LanÄ‰o:
    streamlit run GPT5pro/streamlit_app_eo.py
"""
import time
import pandas as pd
import streamlit as st
from datetime import date, timedelta, datetime

from retradio_lib import (
    ScrapeConfig,
    collect_urls,
    fetch_article,
    _session,
    to_markdown,
    to_text,
    to_csv,
    to_jsonl,
)

st.set_page_config(page_title="Pola Retradio â€“ kolekto kaj kunigo", layout="wide")

st.title("ğŸ“» Pola Retradio â€“ ilo por kolekti kaj kunigi artikolojn")
st.markdown(
    """
Ni aÅ­tomate kolektas la URL-ojn kaj la Ä‰eftekstojn (en Esperanto) de artikoloj publikigitaj Ä‰e https://pola-retradio.org/ dum elektita periodo, kaj kunigas ilin en unu dokumento.
- Metodo: uzas ambaÅ­ RSS/feeds kaj monatajn arkivojn (ÅanÄeblaj laÅ­bezone)
- Eligo: Markdown / TXT / CSV / JSONL (elÅuteblaj)
- Zorgo: respekti robots.txt, malpliigi servilan Åargon, hom-simila UA, prokrasto (throttle)
"""
)

col1, col2, col3 = st.columns(3)
with col1:
    start = st.date_input("Komenca dato", value=(date.today() - timedelta(days=14)))
with col2:
    end = st.date_input("Fina dato", value=date.today())
with col3:
    method = st.selectbox("Kolekta metodo", options=["both", "feed", "archive"], index=0)

throttle = st.slider("Interspaco inter petoj (sek.)", min_value=0.0, max_value=5.0, value=1.0, step=0.1)
max_pages = st.number_input("Limigo de paÄonumerado (None = senlima)", min_value=0, value=0, step=1)
include_audio = st.checkbox("Aldoni ankaÅ­ sonligilojn (nedeviga)", value=False)

if st.button("Ekzekuti la kolekton", type="primary"):
    cfg = ScrapeConfig(
        start_date=start,
        end_date=end,
        method=method,
        throttle_sec=throttle,
        max_pages=(None if max_pages == 0 else int(max_pages)),
        include_audio_links=include_audio,
        use_cache=True,
    )
    with st.spinner("Kolektante URL-ojn..."):
        result = collect_urls(cfg)
    urls = result.urls
    st.success(f"Kandidat-URL-oj: {result.total}")
    st.caption(
        f"feed {result.feed_used}/{result.feed_initial}, archive {result.archive_used}/{result.archive_initial}, "
        f"forigitaj duplikatoj {result.duplicates_removed}, ekskluditaj ekster periodo {result.out_of_range_skipped}"
    )
    if result.earliest_date and result.latest_date:
        st.caption(f"Proksimuma intervalo de publikigo: {result.earliest_date} â€“ {result.latest_date}")
    st.write("La akiro de Ä‰eftekstoj povas postuli iom da tempo. Por redukti la Åargon, ni enmetas prokraston inter petoj.")

    arts = []
    s = _session(cfg)
    progress = st.progress(0.0, "ElÅutante Ä‰eftekstojn...")
    failures = []
    for i, u in enumerate(urls, 1):
        try:
            a = fetch_article(u, cfg, s)
            if a.published and not (cfg.start_date <= a.published.date() <= cfg.end_date):
                pass
            else:
                arts.append(a)
        except Exception as e:
            st.warning(f"Malsukcesis akiri: {u} ({e})")
            failures.append(f"{u} ({e})")
        finally:
            time.sleep(cfg.throttle_sec)
            progress.progress(i / len(urls), f"ElÅutante Ä‰eftekstojn... {i}/{len(urls)}")

    def sort_key(a):
        if a.published:
            pub_naive = a.published.replace(tzinfo=None) if a.published.tzinfo else a.published
            return (pub_naive, a.url)
        return (datetime.max, a.url)

    arts.sort(key=sort_key)
    st.success(f"Pretigita: {len(arts)} artikoloj")
    if not arts:
        st.stop()
    if failures:
        with st.expander("Ne akiritaj eroj (indas reprovi)"):
            for failure in failures:
                st.write(failure)

    df = pd.DataFrame(
        [
            {
                "publikigita": (a.published.strftime("%Y-%m-%d") if a.published else ""),
                "titolo": a.title,
                "URL": a.url,
                "aÅ­toro": a.author or "",
                "kategorioj": ", ".join(a.categories or []),
            }
            for a in arts
        ]
    )
    st.dataframe(df, use_container_width=True, hide_index=True)

    md = to_markdown(arts, cfg)
    txt = to_text(arts)
    csv_str = to_csv(arts)
    jsonl = to_jsonl(arts)

    st.download_button("ğŸ“„ ElÅuti Markdown", md, file_name=f"pola_retradio_{start}_{end}.md", mime="text/markdown")
    st.download_button("ğŸ—’ï¸ ElÅuti TXT", txt, file_name=f"pola_retradio_{start}_{end}.txt", mime="text/plain")
    st.download_button("ğŸ§¾ ElÅuti CSV", csv_str, file_name=f"pola_retradio_{start}_{end}.csv", mime="text/csv")
    st.download_button("ğŸ§° ElÅuti JSONL", jsonl, file_name=f"pola_retradio_{start}_{end}.jsonl", mime="application/json")
else:
    st.info("Elektu komencan kaj finan datojn, poste alklaku â€˜Ekzekuti la kolektonâ€™.")

